\chapter{Problem Analysis}
\label{cha:analysis}

In this chapter we try to analyze our problem in depth and find out different aspects of it. We 
discuss a number of important elements such as possible operation types and we formulate the way
we apply operations on datasets.

There are a number of possible use cases in our problem domain. To demonstrate these cases we assume
we have a number of nodes and datasets, and we need one or more datasets to do certain operations.
In this chapter we explain possible combinations of operations, nodes and datasets.

\section{Operations}
In a final solution there would be many services, some will carry administrative tasks such as
getting a list of currently running tasks, or a list of available datasets. These services do not
change state of the system. It means that even though they could influces the performance of the 
running machine, i.e. with querying the database, they will not make any permanent change into 
data stores or the running instance. We are not insterested in these services.

There are a number of other services who carry the business logic of our application. Calling of
these services will probably change the state of the running instance and might store persistence
data or create new datasets. Moreover they are often data intensive and will trigger some workflows
to begin. We are interested in these services and we call them operations. They could be
any scientific operation, however we do not discuss the detail of them. Instead we are interested on
categorizing them based on their characteristics, such as type and number of required datasets.
\subsection{Types}
We devide data intensive operations into two main groups, the linear operations and non-linear ones.
This is simply comes from the nature of the operation, if it should be applied to input datasets
in parallel or serial. We describe this with a simple algebraic notion.
%In every scenario we have an operation which could be linear or non-linear.

\subsubsection{Linear Operation}
Being linear means that the operation
could be broken into smaller operation and then run in parallel. Having two datasets we can apply the
operation on each of them separately and the aggregate the results. Here is the algebraic notation
of a linear operation which acts on two datasets:

\[ f(a + b) = f(a) + f(b) \]

Being linear or non-linear only matters when we have to operate on more that one dataset, or we want to
break the input into many parts.

Another subtle point here is that for a linear operation we can simply run the operation on the machine that
contains the required dataset, avoiding any dataset transfer amoung different locations in case the requested
dataset is not available locally on the machine which has received the command to start the operation.

\subsubsection{Non-linear Operation}
In contrast to linear there is non-linear operation. This type of operations require all the inputs to be processed
at once. It is not possible to apply the same operation on each of the input datasets and aggregate the result at the
end. This means that these operations could not be run in parallel.
Here is the algebraic notion of such operation:

\[ f(a + b) \neq f(a) + f(b) \]

A sample of non-linear operations is comparison, when it is required to compare all elements on two different datasets. 
Such a use case happens in many areas of science and engineering. These operations are also called All-Pairs. \cite{moretti08}
%This is a common use case with material-science community when applying multi-scale modelling.

In non-linear case the complexity of running operations on multiple remote datasets dramatically increases. When the required
datasets are available on the same machine which starts the operation, there is no problem. However when the datasets are not
available on the same machine or even not on a single remote machine, we have to make at least one data transfer. In this 
case at least one dataset should be moved to the location of the other datasets to make it possible to run the non-linear
operation on one single machine which contains both of required datasets.

\subsection{Data files}
As operations need input and produce output datasets, we have to see how the input data will be provided and where the output
data will be stored. Every input or output needs an explicit address, an endpoint, either local or remote.
Typically users will copy files around and will move them using file transfer tools to a working directory and then will
launch desired script using job scheulders. However they do not pass large data files around and will mention their location
inside their script files, which normally points to some shared ftp storage. 
The scheduler program in turn will run the script at some point of time in future and then
will look for data files inside working directory. Finally any output file will be created in the same directory or a location
explicitely defined in the script.
For each operation we need one or more input datasets which might be available on the same node that wants to run the operation
initially or could reside on other nodes. We desribe some characteristics of these data files shortly.

\subsubsection{Input}
One need to pay attention that we do not pass complete and real datasets as input parameters,
instead we use identifiers to find the required dataset. Currently for most users these identifiers are nothing
than the name of the data files inside the working directory or the explicit path of a dataset on a network machine.
It is assumed that the system has knowledge of available
datasets and can find them prividing an identifier, in this case filename. 
Another point about data files is that they are normally not mission critical and could be reproduced, hence is the emphasizis
on the state. The last point to mention is that input data is not managed by the system.

% TODO: emphaise managed data files, state machine and raft algorith.http://raftconsensus.github.io/

%Therefore we pass kind of unique identifier to data intensiver operations
%(or any operation requiring a dataset as input), not any binary form or data structure containing real data.


\subsubsection{Output}
Operations create output datasets which normally are small in size, therefore we ignore the transfer cost of operation
results in our work. These data files will be normally stored in working directories and are of less importance to us.
In typical environments users check output directory for their result and again they use conventional file transfer
tools to get have that data locally or provide it to some visualization tools to be visualized. However all these steps
are manual and no control and value added services could be built on top of them. Users are not able to track their
activities and there is no history left about them (of course except the server logs), no reports could be made and no
administrative decisions could be made about usages, user activities and so on. This all means that the output data files
are not managed.

%\subsubsection{Locating}
%Every input or output needs an explicit address, an endpoint, either local or remote.

\section{Dataset Identification}
When we ask for an operation and we want to store the result somewhere on the network we have to think about an identifier
for them. 
We need a consistent way of naming datasets. If we ask users to provide result dataset names it will break soon, because we
would have duplicated names. The naming should be managed by system, as well as data management and transfer itself. 
However, we have to provide a user friendly way for naming, one idea is to assign tags to datasets. If a user search for
these tags, any dataset which has that tag will appear in the query results.
Another approach is to store a database of datasets and operations. Having such a database lets the system to make a relation
between datasets, operations and possible other desired factors.

Even though we want to avoid duplicates in our network, it does not mean we do not want redundancy and replication for our 
data, but it means that we rely on other solutions, such as distributed file systems with built-in replication, to do this task.

\subsection{Data Manipulation}
Normally we do not manipulate existing datasets. Each operation results in a number of new datasets. However if we
opt for a storage mechanism such as Hierarchical Data Format (HDF) we might want to store and retrieve datasets from
a single data file presumably in HDF5 format. But it depends on our further design and it does not change the fact
that each operation produces new datasets and we have to store it.


\section{Scenarios}
According to the operation type and number of input datasets, a number of combinations are possible.
In this section we introduce them as scenarios. We begin with a simple one and we gradually add details 
to it and make new scenarios. The following text describes the scenario and it contains expectations of 
my supervisor about the internals of the system, therfore it goes a little into design of the system. 
However in the next chapter we will explain our final approach, therefore any design related material in
this chapter represent only ideas and expectations.%TODO: remove design related material from this chapter

For the following scenarios we assume these genearl statements to be true:
\begin{enumerate}
\item \textbf{The user has neither a prior knowledge where the datasets are stored}
\item \textbf{Nor of how many servers are present on the network}
\end{enumerate}

\subsection{Scneario 1 - Linear operation with one input set}
\label{sc:sc1}
In this scenario we have a linear operation, e.g. \(Op^A\) on \(Node^A\) which
requires one single dataset such as \( Dataset^1 \) which is available on one of the other peers.

%\subsubsection{Assumptions}
We have a distributed network of collaborating servers, where in this case, we consider two computers. 
Each server has its own storage and maintains a number of datasets on it. These servers collaborate 
together to accomplish issued commands. User in this case wants to perform one operation on a dataset
that resides only on one of the servers. 

%\subsubsection{Assumptions}
%There are two main assumptions here:
%\begin{enumerate}
%\item \textbf{The user has neither a prior knowledge where the data is stored}
%\item \textbf{Nor of how many servers are present on the network}
%\end{enumerate}

The user connects to one of the servers, which we call a client. This server is assumed to be part 
of the network, though it may not have any local data stores on it. The user issues, interactively
(or non-interactively) a command on a set of data providing some kind of identification. This command
is broadcasted by the client to all servers in the network. All servers receive this command and check
whether they have the data locally. The server which has the data performs the operation and the others
ignore it. The result of the operation in this case, remains on the same server which the original 
dataset was on. 

\begin{itemize}
\item Note: it is assumed that at any instance of time, only one server acts as a client.
\end{itemize}

Moreover we assume the user has already queried the available data in the entire network by 
issuing something like “list datasets” which outputs dataset names and ids.

The following table shows two servers, each has one dataset. The user is connected to S2.\\

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S1 & DS1 & No \\
S2 & DS2 & Yes \\
\end{tabular}\\

Let us assume the data sets are \(10^6\) random numbers and
the operation is to transform the real random numbers to a set of [0 or 1 ] depending on whether the number is even or odd. 
This operation is assumed to be a user defined method that operates on the data set and 
represents user's intented logic.

\begin{itemize}
\item Note: A dataset can be for example defined as an object that has an id, and a one dimensional array (python list).
\end{itemize}

The user issues the command like this from a python shell: 

\begin{itemize}
\item real2bin(DS1) will result in -> Broadcast(real2bin(DS1))
\item Note: it is assumed that all functions are already defined on all servers, since they execute the same environment.
\end{itemize}

The client broadcasts this function to all servers. 
Each server will check if the dataset with this id exists, if so will run the command. 

This means that each server, especially the client, has to “know about all data sets existing in all servers.
It does not need to have the actual data, but needs to know about it. So that when the user issues the command
above, she does not get a non-existing dataset error from the client, just because the data is not
there. Hence we need some interface, or some wrapper function that checks the argument for the data type, or to 
create some proxy interface from all data to all nodes.

In proposal chapter\ref{cha:proposal} we explain in detail our suggested solution.

\subsection{Scenario 2 - Linear operation with two input sets}
\label{sc:sc2}
This is similar to scenario one, except that the operation requires two datasets to operate. 
In this scenario we need at least three peers involved. We assume the first peer has no
data of our interest therefore it should cooperate with others to accomplish the request. 
Our operation in this case requires two 
different input datasets which are not available on the first peer and we should access them on other peers. 

%\subsubsection{Assumptions}
%The main points the same:
%\begin{enumerate}
%\item \textbf{The user has neither a prior knowledge where the datasets are stored}
%\item \textbf{Nor of how many servers are present on the network}
%\item \textbf{The operation is linear}
%\end{enumerate}

We assume the data distribution is like the following table:

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S0 & --- & Yes \\
S1 & DS1 & No \\
S2 & DS2 & No \\
\end{tabular}\\

\subsection{Scenario 3 - Linear operation with 2+ input sets}
\label{sc:sc3}
This scenario is slightly different than scenario two only about the number of input datasets. All the assumptions and 
requirements remain the same. Except that we would have one more more extra machines containing the rest of datasets.
However we consider the worst case here, where every machine contains only one of the required datasets and the initial 
machine has none of them.
In reality there would be cases than all the data would be available on the same machine or a number of datasets
would exist in one remote machine. The worst data distribution for this case would be like this:

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S0 & --- & Yes \\
S1 & DS1 & No \\
S2 & DS2 & No \\
S3 & DS3 & No \\
\end{tabular}\\

Theoratically there could be more datasets but it is unlikely and often it does not exceed two inputs.

\subsection{Scenario 4 - non-Linear operation with one input set}
With the similar assumptions as before, we have only a different type of operation. With one dataset there is no 
difference between this scenario and first scenario, where the operation is linear. In case this dataset could
be broken into smaller parts we should consider the operation type to prevent any unexpected results.


\subsection{Scenario 5 - non-Linear operation with two input sets}
This case is very complecated. We can not solve it like the previous ones, and we need to make extra decisions.
In this kind of operation we would need both datasets at the same time in one machine in order to produce any
results. Therefore it is not possible to distribute this operation on multiple machines like we can do for 
a linear operation with multiple inputs. 

\subsection{Scenario 6 - non-Linear operation with 2+ input sets}
This is an extension to the previos scenario. 
If we could find a solution for scenario 5, we would extend it to cover this scenario as well. 
There is no fundamental difference between this scneario and the last one. Again we consider the same
data distribution as described for scenario 3.

\subsection{Scenario 7 - mixed operations}
We previously discussed a number of scenarios to run atomic operations on one ore more datasets. The solution to 
above mentioned scenarios will be discussed in detail in the next chapter. However these are the simple cases and
do not cover all possible operations that we need. Here we introduce operations which are composed of another
operation types which we call them \textit{sub-operation}.
Here is the main assumption before mixing operations:

\begin{itemize}
\item \textbf{Any sub-operation will produce in a new dataset}
\end{itemize}

This is necessary in order to simplify the problem and allow us to make some reasonable results
for a limited set of cases and let aside other possiblities for further work. This would be enough
for us to demonstrate our main problems and also to build our basic proposal on top of that.

Here is the algebraic notion of such a problem, \textit{f} and \textit{g} are linear functions:
%http://oeis.org/wiki/List_of_LaTeX_mathematical_symbols
%http://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics
\begin{subequations}
\begin{align*}
\Sigma &= f(a + g(b + c))\\
&= f(a) + g(b + c)\\
&= f(a) + g(b) + g(c)
\end{align*}
\end{subequations}

In the above text we assumed linear operations, however we would have mix of linear and non-linear operations. 
We will discuss this further in next chapter, while proposing a solution to solve such operations.

\section{Decision Making}
The main decision that we need to make at every scenario is whether we should transfer the required data or we
need to delegate the operation to an instance on a node which already has the data. To make a decision we need to
answer a number of questions. First we need to know the location of the data:

\begin{enumerate}
\item What is the operation type?
\item Are required datasets available locally?
\end{enumerate}

These points derive directly from our two main requirements about distributing workflow information and 
eliminating data location. But how these questions serve those purposes?

First of all we need to recognize the type of the operation that we are going to launch. This operation could
resemble any of the discuess scenarios in this chapter. This will make it clear for us if we can directly
jump into distributing the workflow and launching the task or we need to take further steps into breaking
the operation into smaller units. We will discuss this in detail in next chapter, whilc describing our design.

The next import question regarding operations is data availibility. If a local machine has received an operation
and the required dataset is available on locall machine then there is no need for any transfer, then we would need
only to distribute the operation information amoung collaborating peers.

Answers to the above questions will help us to decide whether on which machine we should run the operation. Running an
operation remotely means that we will not transfer back the requested data from another machine, instead we will launch
the operation on the machine containing the data. This is different from conventional approach of transferring methods
or executables to a remote resource and executing it there. In our case we assume that we have the same service API 
available on all the participating machine. Therefore we only need to decide on which machine we have to forward
the request. In case of forwarding or delegating a request to other machines we would need to preserve regarding
workflow information on all the participating machines. This will be discuess in next chapter as part of our
distributed workflow management design.

\iffalse

\subsubsection{Possible Approach}
In order to calculate the result we might take a number of approachers, we start with a combination of \textbf{divide and conquer} and
\textbf{produce-consume-collect} methods.

The S0, in this case, is the peer who receives the command and initiates the request. The two other peers, S1 and S2 respectively, have the required
datasets. The initiator will find the corresponding datasets and will dispatch commands to run each part on each peer and then will collect
the resulting datasets. This will be a blocking operation, we will wait until the other peers finish their parts and return the result
to us. If the output is a number it will be returned to the user, if it is a dataset it will be stored based on defined storage mechanism, 
currently we use random storage. The peer will break the operation into smaller operations each one calculating result for one of datasets, 
this \textbf{sub-operations} will be executed like \textbf{scenario 1} and the result will be collected by initiator peer.

We assume that in this case we have two arrays, each consisting of \(10^6\) random numbers. We have to first transform these datasets into
a set of [0 or 1] based on the number being even or odd (use case 1) and then we make a third dataset which contains the sum of every two
corresponding numbers in range of [0 to 2].


\begin{itemize}
\item Note: in this case each pear should be able to run the requested linear operation on one or more datasets.
\end{itemize}

The notation of above mentioned approach will be like this:

\[ Operation(A + B) = Operation(A) + Operation(B) \]

In order to run this operation in a collective way, we need to think of the type of service calls in our system, whether they are blocking or
non-blocking. Since often the operations in HPC environments are time consuming and long-running, we consider the non-blocking approach. In
this way the user will provide a dataset name for storing the result. The operation will be \textbf{submitted} to the collaborative network.
Later on user is able to query for the result using the key that she had provided at the time of submission. This allows us to design our system
in a more decentralized way, where each peer can inform others (neighbors) about a request in a \textbf{publish-subscribe} manner, where the peer
will publish a request and finish the operation. Later on the peer who has the dataset will \textbf{react} to the published request and will take
further actions, all the other peers who do not have the requirements (the dataset for now) will ignore it, however they can store the details of 
running operations for next steps, when we will come to more complex workflow.

To show more detailed version of this operation we demonstrate the steps for it:

\begin{enumerate}
\item User issues the command to S0, providing DS1, DS2 \st{and a unique name for the result}
\item System will check whether the operation is linear
\item Then it will break the command into sub-commands, each for one of datasets
\item System will generate unique ids for each sub-command
\item System will then submit the sub-commands along with dataset name and the unique id for the result dataset
to \textbf{itself}, which will cause a situation like scenario 1
\item System will next have to collect the results in a non-blocking manner which we will discuss shortly.
\end{enumerate}

\begin{itemize}
\item With the use of operation ids we eliminate the need to get a result dataset name from user but we still can accept \textbf{tags} from users.
\end{itemize}

\begin{itemize}
\item We assume every operation involving more than one dataset is made of other operations which are already defined in the system.
\end{itemize}

There is an important issue here, we create sub-operations for each operation and we run them in a non-blocking manner, this will
cause it almost impossible to return the result of operation to the user in one run. One might think that we can block and query
until the result of sub-operations are ready, but this is something that we want to avoid. Therefore to solve this issue in a 
distributed manner, we introduce an operation id for each user request. We inform all the peers via sending messages (signals) about
the new operation and it is id and sub-commands. Each peer will update this operation internally based on further received messages.
We also return the operation id to the user instead of any results. Then user will query for the result of operation, providing the 
operation id. We change the above steps like this:

\begin{enumerate}
\item User issues the command to S0, providing DS1, DS2 and a unique name for the result
\item \textbf{System will generate a unique id for the operation and will store it along with the parameters}
\item System will check whether the operation is linear
\item Then it will break the command into sub-commands, each for one of datasets
\item System will generate unique ids for each sub-command
\item \textbf{System will notify other peers about the incoming operation with related parameters}
\item System will then submit the sub-commands along with dataset name and the unique id for the result dataset
to \textbf{itself}, which will cause a situation like scenario 1
\item System will next have to collect the results in a non-blocking manner which we will discuss shortly.
\item System will return the operation id to the user
\end{enumerate}

In the other hand the other peers which are the same basically, will react to the new operation signal:

\begin{enumerate}
\item Receive operation update message
\item Make a local lookup if the operation should be added or updated
\item Add or update the operation in the local storage
\end{enumerate}

Having the operation id and local updating storage for operations we now need to find a way to collect the results.
First of all we need to decide which peer will collect the results. We take the most straight forward for now, the 
initiator peer, which has the knowledge of existing datasets in the network along with their sizes, will pick the 
peer which contains the largest dataset as the collector peer. We explicitly decide about the collector node in the
beginning either by size or randomly amount the data container peers.

It is worthy to mention that the collector peer will then store the result based on the configured storage mechanism 
which is random storage for now, not necessarily storing on the same node.

Now we have enough information in each peer to collect, process and store the results. The peers (including the collector)
 will react to operation methods like this:

\begin{enumerate}
\item Receive operation update message
\item Make a local lookup if the operation should be added or updated
\item Add or update the operation in the local storage
\item Am I the collector? If yes do the followings:
\begin{itemize}
\item check if the sub-operations are done
\item If the sub-operations are done, collect their results
\item Process the results
\item Based on the storage mechanism store the result
\item Update the operation with the result dataset id
\item Change status of operation to "done" (we need a proper state-machine here)
\item Inform other peers about the update
\end{itemize}
\end{enumerate}

Now if user makes a query giving the operation id this would be the result:

\begin{enumerate}
\item Check operation storage
\item If the operation is marked done, return the dataset id
\item If it is not done, return the status.
\end{enumerate}
\fi