\chapter{Proposed Solution}
\label{cha:proposal}
In this chapter we explain how we can solve the scenarios which we explained in previous chapter.
We introduced seven different scenarios, three of them for linear operations and three of them for non-linear
ones, and one mixed operation type. We begin with the most basic scenario, solving a data intensive operation
which requires one dataset. Then we will extend it to accept more datasets and we will sove them. We will
cover only solving linear operations and non-linear operations will be left for further work.

\section{Big Picture}
To make it easier to consume all this text we will make an effort to show a big picture of our desgin beforehand. Here it is.

There would be multiple computers, our application installed on each of them. This machines do not depend on each
other to operate. Each of them runs the same program instance as others. Each one has the same set of services that
other instances and users can call. Each instance has a number of datasets. These peers will exchange knowledge of
existing datasets with eachother. Each instance contains atleast two distributed internal structures,
we call them \textit{distributed stores}, these are similar to distributed key/value stores.
One store for operations, and another one for datasets. When an instance receives a service call for an operation, it
will store some basic information and identification about it inside its operation store. This store automatically will
inform other peers about these changes, so others will have the same knowledge afterwards.

Upon an incoming request, an instance would be able to accomplish the task alone, only if it has access to datasets locally.
Otherwise it will do nothing and will simply ignore it. But since the operation store will distribute this information
seamlessly, any other peer has the opportunity to laucn the operation if they have the desired data, if it is not the case
they will only update their internal store and do nothing. But if they had the data, they would run the desired operation
and will update the state of the operation to a meaningful one, such as \textit{processing}, and will distribute it
accordingly to inform others about the new state of this operation.

In case of a mixed operation the receiver will break it into smaller operations and will self-launch them accordingly and
will register a meta operation and will assign the afore-mentioned smaller operations as sub/child operations. We will
also register this meta operation into the internal operation storage will then automatically distribute it like any other
operation. This way we behave the same way with simple or complicated operations and we use the same interface to interact
with them.

In case of operations which have a number of child operations there would be a need to aggregate the result of sub operations.
This would be done in a seamless way as well. We will introduce a \textit{collector} peer which will randomly take control
and collect the results.

When an operation is done, the result dataset will be given to the internal distributed dataset store of the responsible
peer - the peer which is running it - and it will be stored in a backend storage but the unique identifer of the dataset
will be distributed and other peers will get the knowledge of a new dataset and the container peer respectively. 
To query results users have to use the operation id that they have received upon the initial service call. 

This way we can apply a collaboration technique to a number of autonomus peers. 
This design allows us to have peers which are able to work independantly, 
but meanwhile are member of a larger network of peers and participate in accomplishing larger tasks.

We will cover these in more detail during this chapter.

\section{Basic Idea}
The basic idea that we will follow in this chapter, relies on breaking the operations into smaller units which
we can solve them in one step, such as only one operationor service call. 
In our design the simple operations are the building blocks for mixed ones.
We build them on top of the atomic units, which we know how to solve them.
This idea has the advantage of allowing us to reuse our work and decrease the complexitly of implementing
more complicated operatins. However we would have increased complexity in messaging parts.

We assume that we have the information about the datasets
available on all machines i.e. in form of a distributed table
with entries containing the node address and dataset id. Based on this
information the application can decide if it has the required data or
not. We will explain this in detail later in this and next chapter.

Based on this algorithm the application implicitly delegates operations to the other 
nodes (instances of the same program), where the data is available. 
It would be a non-blocking service call, 
just like signaling others about an incoming request.
Along with any operation change, the distributed workflow manager will synchronize the information amoung the peers.
Any change in datasets in any collaborative node will also be synchronized with peers and will be added to 
a distributed list.

\subsection{Break and Conquer - Recursive Call}
This makes the high level algorithm that we using in this work. We break operations into smallest possible operations
and we implement them. Then we build other operations using these small units. For example we solve scenario number one, as
discussed in chapter \ref{cha:schenario} to run one single operation on a single dataset.
In order to run it successfully we need to find the corresponding dataset and in case it is not available locally we have
to launch the operation on the node which has it. Then when we come to scenrio number two, i.e. applying the same operation
on two datasets we break it into two smaller units, and one \textit{meta operation}.

After having two smaller operations and one meta operation, we launch the smaller operations implicitly. Actually we break
scenario number two into two instances of scenario one and one meta operation to observe the overall process. The implementaion
of this process will be presented in the prototype.

\subsection{Non-blocking Calls}
We base our desing on non-blocking service calls. This means all the calls in our system would be asynchronous.
When a user calls a servie, she would instantly receive an id, instead of being blocked for the real result.
The rationale here is because of possible long-execution times in our use cases. 
One needs to think of a service call in our design as \textit{request submission}. 
Not only the interaction between user and peers is asynchronous but the inter-peer service calls follow the same path.
Any service call regarding running an operation would be non-blocking and will result in an unique identifier.

\subsection{Dataset identification}
When a user or peer wants to submit a request for an operation, they would not provide a real dataset as input.
They would instead provide the unique id of a dataset existing on our network of collaborative peers.
The our system, i.e. the peer who has received the request, will look at its internal dataset store to see if it
has the dataset locally or not.
This will happen in the pre-processing step. 
In any case a signal will be dispatched to other peers about the new operation.
The nature of these signals are also non-blocking.

\subsection{Distributed Operation}
To realize the above mentioned method, we need to distribute any single operation. To achieve this we assign one unique id
to every incoming operation. This will happen before doing any real work on the request. In our prototype we have implemented
this with \textit{decorators} in python programming language. 
From this point of time, 
the operation will be known and tracked with this id. One can imagine this as a ticket which
allows monitoring every change made to an operation. 
Apart from id we will store name of the operation and its input datasets. 
This will allow us to relaunch this operation in case we need to. 
The store that keeps this information is distributed amoung all participating nodes. 
Any further infromation such as results will be attached to this store during the process. 
There are concerns running a distributed store that needs further attention and 
we try to cover them in further discussions and futher work.

\subsection{Distributed stores}
We will use this term many times in the next sections, therefore we have to explain it.
Basically we talk about a simple key/value storage. 
Currently the storage mechanism is not important for us, it could be memory or anything else.
This stores are like dictionaries, the keys would be the unique identifers.
Either id of an operation or id of a dataset. 
Then we will store futher information about that object as the value for that key.
We will take advantage of very simple structure to make it easy to be exchanged among peers.

From one side, these stores would be simple repositories to read/write key/value pairs.
This simplifies dependant parts.
From another point of view these stores are distributed objects, but not really.
The keep sending signals about any change in their internals.
These signals will be catched and handled buy another component respectively.
The other component will then signal other peers about certain changes that has been happened in this store.
Other peers then will catch this message and will unpack the message and will updted their own storages.

This way with minimum coupling we would have a distributed storage which its distributed nature is hidden from
the objects which need to use it.

\subsection{Collectors}
Nature of simple operations is simple. 
An operation either will be handled locally or a dispatched signal will be handled
by a peer who has the requested data. 
The status and result dataset id will be stored inside the operation store under the operation's unique id.



\subsection{Unique IDs}


\section{Operation Types}
\subsection{Simple Operation}
\subsection{Mixed Operation}

\section{Using Prior Art}
\subsection{Data Transfer}
We can take advantage of existing Distributed File Systems
(DFS) to make the data available for operations. We can then eliminate
the complexity of data transfer between these two nodes and delegate it
to existing distributed file systems. The main point is we don't rely on
DFS for all of our decision making part but we explicitly make the 
decision which operations to run on specific nodes and then for the 
data transfer part we can use a meta or universal disk concept to deliver the
remaining data.


% ==============================================
% ==============================================
\section{Proposed Approach}
In order to calculate the result we might take a number of approachers, we start with a combination of \textbf{divide and conquer} and
\textbf{produce-consume-collect} methods.

The S0, in this case, is the peer who receives the command and initiates the request. The two other peers, S1 and S2 respectively, have the required
datasets. The initiator will find the corresponding datasets and will dispatch commands to run each part on each peer and then will collect
the resulting datasets. This will be a blocking operation, we will wait until the other peers finish their parts and return the result
to us. If the output is a number it will be returned to the user, if it is a dataset it will be stored based on defined storage mechanism, 
currently we use random storage. The peer will break the operation into smaller operations each one calculating result for one of datasets, 
this \textbf{sub-operations} will be executed like \textbf{scenario 1} and the result will be collected by initiator peer.

We assume that in this case we have two arrays, each consisting of \(10^6\) random numbers. We have to first transform these datasets into
a set of [0 or 1] based on the number being even or odd (use case 1) and then we make a third dataset which contains the sum of every two
corresponding numbers in range of [0 to 2].


\begin{itemize}
\item Note: in this case each pear should be able to run the requested linear operation on one or more datasets.
\end{itemize}

The notation of above mentioned approach will be like this:

\[ Operation(A + B) = Operation(A) + Operation(B) \]

In order to run this operation in a collective way, we need to think of the type of service calls in our system, whether they are blocking or
non-blocking. Since often the operations in HPC environments are time consuming and long-running, we consider the non-blocking approach. In
this way the user will provide a dataset name for storing the result. The operation will be \textbf{submitted} to the collaborative network.
Later on user is able to query for the result using the key that she had provided at the time of submission. This allows us to design our system
in a more decentralized way, where each peer can inform others (neighbors) about a request in a \textbf{publish-subscribe} manner, where the peer
will publish a request and finish the operation. Later on the peer who has the dataset will \textbf{react} to the published request and will take
further actions, all the other peers who do not have the requirements (the dataset for now) will ignore it, however they can store the details of 
running operations for next steps, when we will come to more complex workflow.

To show more detailed version of this operation we demonstrate the steps for it:

\begin{enumerate}
\item User issues the command to S0, providing DS1, DS2 \st{and a unique name for the result}
\item System will check whether the operation is linear
\item Then it will break the command into sub-commands, each for one of datasets
\item System will generate unique ids for each sub-command
\item System will then submit the sub-commands along with dataset name and the unique id for the result dataset
to \textbf{itself}, which will cause a situation like scenario 1
\item System will next have to collect the results in a non-blocking manner which we will discuss shortly.
\end{enumerate}

\begin{itemize}
\item With the use of operation ids we eliminate the need to get a result dataset name from user but we still can accept \textbf{tags} from users.
\end{itemize}

\begin{itemize}
\item We assume every operation involving more than one dataset is made of other operations which are already defined in the system.
\end{itemize}

There is an important issue here, we create sub-operations for each operation and we run them in a non-blocking manner, this will
cause it almost impossible to return the result of operation to the user in one run. One might think that we can block and query
until the result of sub-operations are ready, but this is something that we want to avoid. Therefore to solve this issue in a 
distributed manner, we introduce an operation id for each user request. We inform all the peers via sending messages (signals) about
the new operation and it is id and sub-commands. Each peer will update this operation internally based on further received messages.
We also return the operation id to the user instead of any results. Then user will query for the result of operation, providing the 
operation id. We change the above steps like this:

\begin{enumerate}
\item User issues the command to S0, providing DS1, DS2 and a unique name for the result
\item \textbf{System will generate a unique id for the operation and will store it along with the parameters}
\item System will check whether the operation is linear
\item Then it will break the command into sub-commands, each for one of datasets
\item System will generate unique ids for each sub-command
\item \textbf{System will notify other peers about the incoming operation with related parameters}
\item System will then submit the sub-commands along with dataset name and the unique id for the result dataset
to \textbf{itself}, which will cause a situation like scenario 1
\item System will next have to collect the results in a non-blocking manner which we will discuss shortly.
\item System will return the operation id to the user
\end{enumerate}

In the other hand the other peers which are the same basically, will react to the new operation signal:

\begin{enumerate}
\item Receive operation update message
\item Make a local lookup if the operation should be added or updated
\item Add or update the operation in the local storage
\end{enumerate}

Having the operation id and local updating storage for operations we now need to find a way to collect the results.
First of all we need to decide which peer will collect the results. We take the most straight forward for now, the 
initiator peer, which has the knowledge of existing datasets in the network along with their sizes, will pick the 
peer which contains the largest dataset as the collector peer. We explicitly decide about the collector node in the
beginning either by size or randomly amount the data container peers.

It is worthy to mention that the collector peer will then store the result based on the configured storage mechanism 
which is random storage for now, not necessarily storing on the same node.

Now we have enough information in each peer to collect, process and store the results. The peers (including the collector)
 will react to operation methods like this:

\begin{enumerate}
\item Receive operation update message
\item Make a local lookup if the operation should be added or updated
\item Add or update the operation in the local storage
\item Am I the collector? If yes do the followings:
\begin{itemize}
\item check if the sub-operations are done
\item If the sub-operations are done, collect their results
\item Process the results
\item Based on the storage mechanism store the result
\item Update the operation with the result dataset id
\item Change status of operation to "done" (we need a proper state-machine here)
\item Inform other peers about the update
\end{itemize}
\end{enumerate}

Now if user makes a query giving the operation id this would be the result:

\begin{enumerate}
\item Check operation storage
\item If the operation is marked done, return the dataset id
\item If it is not done, return the status.
\end{enumerate}
